{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f27d861b-085b-4341-9c3d-a3a7579b5301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72794777-41f6-4c02-bd57-ff4438be99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLayer(nn.Module):\n",
    "    def __init__(self, lambd):  #run when object of LambdaLayer is initiaed\n",
    "        super(LambdaLayer, self).__init__() #initialize object\n",
    "        self.lambd = lambd\n",
    "        #applies lamdbda function\n",
    "    \n",
    "    def forward(self, x):  # x = input tensor\n",
    "        return self.lambd(x)\n",
    "\n",
    "\n",
    "class BasicConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, option='A'):  # option A - default shortcut connection\n",
    "        super(BasicConvBlock, self).__init__()\n",
    "        #initialization of Block object\n",
    "\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(out_channels)\n",
    "        #defince components of convolution block\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "        #define shortcut connection for ResNet architecture\n",
    "        \n",
    "\n",
    "\n",
    "        if stride != 1 or in_channels != out_channels:  #conditions meant --> shortcut needs to be adjusted\n",
    "            pad_channels = out_channels // 4\n",
    "            self.shortcut = LambdaLayer(lambda x: F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, pad_channels, pad_channels, 0, 0)))\n",
    "            #alters padding of input tensor\n",
    "            #dimensions of the tensor align during forwards passing\n",
    "        \n",
    "    def forward(self, x):  #actual passing of data and application of block parameters\n",
    "        out = self.conv1(x)\n",
    "        out = self.batch_norm1(out)\n",
    "        out = self.activation1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.batch_norm2(out)\n",
    "\n",
    "        shortcut_out = self.shortcut(x)\n",
    "        out += shortcut_out\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c60621-a190-4480-a48c-37da5e5a5b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Network(nn.Module):  #define network\n",
    "    \n",
    "    def __init__(self, block_type, block_num):\n",
    "        super(Network, self).__init__()\n",
    "        self.in_channels = 16  # no of input channels\n",
    "        \n",
    "        self.conv0 = self._conv_block(3, 16, kernel_size=3, stride=1, padding=1, bias=False)#layer 0:performs conv operation, batch, RELU \n",
    "                                                                                            \n",
    "        self.blocks = nn.ModuleList()  #list for block layers\n",
    "        self.blocks.append(self._layer_block(block_type, 16, block_num[0], starting_stride=1))\n",
    "        self.blocks.append(self._layer_block(block_type, 32, block_num[1], starting_stride=2))\n",
    "        self.blocks.append(self._layer_block(block_type, 64, block_num[2], starting_stride=2))\n",
    "        #adds specific number of blocks for each stage of the network to the list\n",
    "\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.linear = nn.Linear(64, 10) #64-dimensional input --> 10-dimensional output\n",
    "\n",
    "    def _conv_block(self, in_channels, out_channels, **kwargs): #helper method to create convolutional block\n",
    "        return nn.Sequential( #defines the sequential operations of the block\n",
    "            nn.Conv2d(in_channels, out_channels, **kwargs), #2d convolution\n",
    "            nn.BatchNorm2d(out_channels), #batch normalization\n",
    "            nn.ReLU(inplace=True) \n",
    "        )\n",
    "\n",
    "    def _layer_block(self, block_type, out_channels, block_num, starting_stride):  #creates block layer\n",
    "        strides_list = [starting_stride] + [1] * (block_num - 1)\n",
    "        layers = []\n",
    "        \n",
    "        for stride in strides_list:\n",
    "            layers.append(block_type(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "\n",
    "        return nn.Sequential(*layers) #creates all the block instances\n",
    "\n",
    "    def forward(self, x): #forward pass of data\n",
    "        x = self.conv0(x)\n",
    "        for block in self.blocks: #list of layers of the network \n",
    "            x = block(x)\n",
    "        x = self.avgpool(x) #adaptive average pooling --> fixed-size representation\n",
    "        x = torch.flatten(x, 1) #collapes tensor dimenion (except for batch)\n",
    "        x = self.linear(x) #linear transformation to tensor that creates comprehensenable result\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71edc0a1-4af0-409e-a62a-35190deed772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18_test():\n",
    "    return Network(block_type = BasicConvBlock , block_num = [2,2,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffd3438-2307-4f8f-9922-5cc43a5ebac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 32, 32]             432\n",
      "       BatchNorm2d-2           [-1, 16, 32, 32]              32\n",
      "              ReLU-3           [-1, 16, 32, 32]               0\n",
      "            Conv2d-4           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-5           [-1, 16, 32, 32]              32\n",
      "              ReLU-6           [-1, 16, 32, 32]               0\n",
      "            Conv2d-7           [-1, 16, 32, 32]           2,304\n",
      "       BatchNorm2d-8           [-1, 16, 32, 32]              32\n",
      "          Identity-9           [-1, 16, 32, 32]               0\n",
      "   BasicConvBlock-10           [-1, 16, 32, 32]               0\n",
      "           Conv2d-11           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-12           [-1, 16, 32, 32]              32\n",
      "             ReLU-13           [-1, 16, 32, 32]               0\n",
      "           Conv2d-14           [-1, 16, 32, 32]           2,304\n",
      "      BatchNorm2d-15           [-1, 16, 32, 32]              32\n",
      "         Identity-16           [-1, 16, 32, 32]               0\n",
      "   BasicConvBlock-17           [-1, 16, 32, 32]               0\n",
      "           Conv2d-18           [-1, 32, 16, 16]           4,608\n",
      "      BatchNorm2d-19           [-1, 32, 16, 16]              64\n",
      "             ReLU-20           [-1, 32, 16, 16]               0\n",
      "           Conv2d-21           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-22           [-1, 32, 16, 16]              64\n",
      "      LambdaLayer-23           [-1, 32, 16, 16]               0\n",
      "   BasicConvBlock-24           [-1, 32, 16, 16]               0\n",
      "           Conv2d-25           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-26           [-1, 32, 16, 16]              64\n",
      "             ReLU-27           [-1, 32, 16, 16]               0\n",
      "           Conv2d-28           [-1, 32, 16, 16]           9,216\n",
      "      BatchNorm2d-29           [-1, 32, 16, 16]              64\n",
      "         Identity-30           [-1, 32, 16, 16]               0\n",
      "   BasicConvBlock-31           [-1, 32, 16, 16]               0\n",
      "           Conv2d-32             [-1, 64, 8, 8]          18,432\n",
      "      BatchNorm2d-33             [-1, 64, 8, 8]             128\n",
      "             ReLU-34             [-1, 64, 8, 8]               0\n",
      "           Conv2d-35             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-36             [-1, 64, 8, 8]             128\n",
      "      LambdaLayer-37             [-1, 64, 8, 8]               0\n",
      "   BasicConvBlock-38             [-1, 64, 8, 8]               0\n",
      "           Conv2d-39             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-40             [-1, 64, 8, 8]             128\n",
      "             ReLU-41             [-1, 64, 8, 8]               0\n",
      "           Conv2d-42             [-1, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-43             [-1, 64, 8, 8]             128\n",
      "         Identity-44             [-1, 64, 8, 8]               0\n",
      "   BasicConvBlock-45             [-1, 64, 8, 8]               0\n",
      "AdaptiveAvgPool2d-46             [-1, 64, 1, 1]               0\n",
      "           Linear-47                   [-1, 10]             650\n",
      "================================================================\n",
      "Total params: 172,506\n",
      "Trainable params: 172,506\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 3.44\n",
      "Params size (MB): 0.66\n",
      "Estimated Total Size (MB): 4.11\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet18_test()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "resnet.to(device)\n",
    "summary(resnet, (3,32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffb845c-f171-4161-964d-6832048420da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    transform = transforms.Compose([transforms.Resize((224,2244)),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize(mean=[0.5,0.5,0.5], std = [0.5,0.5,0.5])])\n",
    "    path = \"C:/Users/alexc/IB1/Caltech/caltech-101/caltech-101\"\n",
    "    dataset = ImageFolder(path, transform=transform)\n",
    "\n",
    "    train = int(0.8 * len(dataset)) #training data\n",
    "    test = len(dataset) - train #testing data\n",
    "    train , test = random_split(dataset, (train, test)) \n",
    "\n",
    "    print(\"Training Images:  {} \".format(len(train)))\n",
    "    print(\"Testing Images: {} \".format(len(test)))\n",
    "\n",
    "    Batch_size = 32\n",
    "\n",
    "    trainLoader = DataLoader(train, batch_size = Batch_size, shuffle = True)\n",
    "    testLoader = DataLoader(test, batch_size = Batch_size, shuffle = True)\n",
    "    #provide iterables\n",
    "    \n",
    "    return trainLoader, testLoader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22ca4551-a0c2-49c1-b010-517a05a52372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Images:  7315 \n",
      "Testing Images: 1829 \n"
     ]
    }
   ],
   "source": [
    "trainLoader, testLoader = data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2af983b7-6a3c-41c1-a759-f2b1082cc936",
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2154706-b8dc-486a-b981-8642000e3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet():\n",
    "    epochs = 15\n",
    "    train_samples_num = 45000\n",
    "    val_samples_num = 5000\n",
    "    train_costs, val_costs = [], [] #to store training and validation losses\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        resnet.train()\n",
    "        train_running_loss = 0\n",
    "        correct_train = 0\n",
    "\n",
    "        for inputs, labels in trainLoader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad() #optimizer set to 0\n",
    "            \n",
    "            #start forwading data\n",
    "            prediction = resnet(inputs)\n",
    "\n",
    "            loss = crit(prediction, labels)\n",
    "\n",
    "            #backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_outputs = torch.max(prediction.data, 1)\n",
    "            correct_train += (predicted_outputs == labels).sum().item()\n",
    "            #calculates number of correctly predicted trainign samples\n",
    "\n",
    "        train_epoch_loss = train_running_loss / train_samples_num #avg training loss\n",
    "        train_costs.append(train_epoch_loss)\n",
    "        train_acc = correct_train / train_samples_num #accuracy\n",
    "\n",
    "        resnet.eval() #evaluation mode - dropout and batch normalization are disabled\n",
    "        val_running_loss = 0\n",
    "        correct_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testLoader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            \n",
    "                prediction = resnet(inputs)\n",
    "\n",
    "                loss = crit(prediction, labels)\n",
    "\n",
    "                val_running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted_outputs = torch.max(prediction.data, 1)\n",
    "                correct_val += (predicted_outputs == labels).sum().item()\n",
    "\n",
    "        val_epoch_loss = val_running_loss / val_samples_num\n",
    "        val_costs.append(val_epoch_loss)\n",
    "        val_acc = correct_val / val_samples_num\n",
    "\n",
    "        info = \"[Epoch {}/{}]: train-loss = {:0.6f} | train-acc = {:0.3f} | val-loss = {:0.6f} | val-acc = {:0.3f}\" \n",
    "        print(info.format(epoch+1, epochs, train_epoch_loss, train_acc, val_epoch_loss, val_acc)) #training process\n",
    "\n",
    "        torch.save(resnet.state_dict(), '/content/checkpoint_gpu_{}'.format(epoch + 1)) #saves dictionary of trained model\n",
    "\n",
    "    torch.save(resnet.state_dict(), '/content/resnet-18_weights_gpu') #final trained model dictionary is saved\n",
    "\n",
    "    return train_costs, val_costs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69855e1-b316-4725-a76e-e9662f871dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
